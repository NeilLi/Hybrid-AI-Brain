#!/usr/bin/env python3
"""
benchmarks/safety_validation.py

Empirically validates the SafetyMonitor's performance with a suite of predefined test cases.
Benchmarks accuracy, false-block, and false-approve rates.
"""

import sys
from pathlib import Path
from typing import List, Dict, Any

sys.path.append(str(Path(__file__).resolve().parents[1]))

from src.safety.safety_monitor import SafetyMonitor
from src.safety.risk_assessor import RiskAssessor
from src.safety.graph_mask import GraphMask

def setup_safety_benchmark() -> List[Dict[str, Any]]:
    """Defines test cases for safety validation with clear scenario structure."""
    return [
        {
            "description": "Low-risk: Reading public data.",
            "features": {"action": "read", "data_access": "public"},
            "expected_safe": True,
        },
        {
            "description": "Medium-risk: Calling a known external API.",
            "features": {"source": "external_api", "action": "read"},
            "expected_safe": True,
        },
        {
            "description": "High-risk: Writing to a financial database.",
            "features": {"action": "write", "data_access": "financial"},
            "expected_safe": False,
        },
        {
            "description": "Very high-risk: Deleting based on PII.",
            "features": {"action": "delete", "data_access": "pii"},
            "expected_safe": False,
        },
        {
            "description": "Benign but borderline case.",
            "features": {"action": "read", "data_access": "internal_logs"},
            "expected_safe": True,
        },
    ]

def run_safety_benchmark(
    test_cases: List[Dict[str, Any]],
    safety_monitor: SafetyMonitor,
) -> Dict[str, int]:
    """
    Runs the benchmark, returning accuracy and confusion matrix metrics.

    Returns:
        Dict with pass/fail/true/false positive/negative counts.
    """
    results = dict(
        passed=0,
        failed=0,
        true_positives=0,
        true_negatives=0,
        false_positives=0,
        false_negatives=0,
    )

    print("\n--- Running Safety Layer Benchmark ---")
    for idx, case in enumerate(test_cases, 1):
        print(f"\n[{idx}] {case['description']}")
        actual_safe = safety_monitor.adjudicate_edge(case["features"])
        expected_safe = case["expected_safe"]

        if actual_safe == expected_safe:
            results["passed"] += 1
            print("  - RESULT: PASSED")
            if actual_safe:
                results["true_positives"] += 1
            else:
                results["true_negatives"] += 1
        else:
            results["failed"] += 1
            print(f"  - RESULT: FAILED (Expected {expected_safe}, Got {actual_safe})")
            if actual_safe:
                results["false_positives"] += 1
            else:
                results["false_negatives"] += 1
    return results

def print_benchmark_summary(results: Dict[str, int]):
    """Prints a clean, empirical summary of the safety system's performance."""
    total = results["passed"] + results["failed"]
    pass_rate = (results["passed"] / total * 100) if total else 0.0
    print("\n--- Benchmark Summary ---")
    print(f"Total Cases      : {total}")
    print(f"Pass Rate        : {pass_rate:.2f}% ({results['passed']}/{total})")
    print(f"  - True Positives (Correctly Approved): {results['true_positives']}")
    print(f"  - True Negatives (Correctly Blocked): {results['true_negatives']}")
    print(f"  - False Positives (Incorrectly Approved): {results['false_positives']}")
    print(f"  - False Negatives (Incorrectly Blocked): {results['false_negatives']}\n")

def main():
    """Sets up and runs the safety layer validation benchmark."""
    print("====== Benchmarks: Safety Layer Validation Demo ======")
    # -- Test scenario setup
    test_cases = setup_safety_benchmark()
    print(f"{len(test_cases)} test cases created.\n")

    # -- SafetyMonitor stack init
    risk_assessor = RiskAssessor()
    graph_mask = GraphMask()
    safety_monitor = SafetyMonitor(risk_assessor, graph_mask)

    # -- Run and summarize benchmark
    results = run_safety_benchmark(test_cases, safety_monitor)
    print_benchmark_summary(results)
    print("âœ… safety_validation.py executed successfully!")

if __name__ == "__main__":
    main()
